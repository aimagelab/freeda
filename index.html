<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="FreeDA: Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation">
  <meta name="keywords" content="FreeDA, Open-Vocabulary, Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FreeDA: Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation
  </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="./static/images/favicon.png" width="50" height="50"></img> <span
                style="color: #FF0078;">Free</span><span style="color: #00509A;">DA</span>
            </h1>
            <h1 class="title is-2 publication-title">Training-Free Open-Vocabulary Segmentation with
              Offline Diffusion-Augmented Prototype Generation</h1>
            <h1 class="title is-4" style="color: #5c5c5c;">CVPR 2024</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://lucabarsellotti.github.io/">Luca Barsellotti</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.robertoamoroso.it/">Roberto Amoroso</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=90">Marcella
                  Cornia</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.lorenzobaraldi.com/">Lorenzo Baraldi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://aimagelab.ing.unimore.it/imagelab/person.asp?idpersona=1">Rita
                  Cucchiara</a><sup>1,2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Modena and Reggio Emilia,</span>
              <span class="author-block"><sup>2</sup>IIT-CNR</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">* Equal contribution</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="static/pdf/Training_Free_Open_Vocabulary_Segmentation_with_Offline_Diffusion_Augmented_Prototype_Generation.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.06542" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/aimagelab/freeda" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body" style="padding-bottom: 10px;">
        <img src="static/images/into_the_wild.png" class="center">
      </div>
    </div>
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          <strong>FreeDA</strong> is a training-free approach to perform open-vocabulary segmentation with free-form
          textual queries
        </h2>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Open-vocabulary semantic segmentation aims at segmenting arbitrary categories expressed in textual form.
              Previous works have trained over large amounts of image-caption pairs to enforce pixel-level multimodal
              alignments. However, captions provide global information about the semantics of a given image but lack
              direct localization of individual concepts. Further, training on large-scale datasets inevitably brings
              significant computational costs.
              In this paper, we propose <strong>FreeDA</strong>, a training-free diffusion-augmented method for
              open-vocabulary semantic
              segmentation, which leverages the ability of diffusion models to visually localize generated concepts and
              local-global similarities to match class-agnostic regions with semantic classes. Our approach involves an
              offline stage in which textual-visual reference embeddings are collected, starting from a large set of
              captions and leveraging visual and semantic contexts. At test time, these are queried to support the
              visual matching process, which is carried out by jointly considering class-agnostic regions and global
              semantic similarities.
              Extensive analyses demonstrate that <strong>FreeDA</strong> achieves <strong>state-of-the-art</strong>
              performance on five datasets,
              surpassing previous methods by more than <strong>7.0 average points in terms of mIoU and without requiring
                any
                training.</strong>
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      </br>
      <!-- Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <h2 class="title is-5">Diffusion-Augmented Prototype Generation</h2>
          <div class="content has-text-centered">
            <img src="static/images/diffusion-augmented_prototype_generation.png" class="center" width="350">
            </br></br>
            <strong>Overview of the diffusion-augmented prototype generation phase</strong>. Visual prototypes are
            generated by pooling self-supervised visual features on weak localization masks extracted from Stable
            Diffusion.
          </div>
          </br>
          <h2 class="title is-5">Training-Free Mask Prediction</h2>
          <div class="content has-text-centered">
            <img src="static/images/training-free_mask_prediction.png" class="center">
            </br></br>
            <strong>Overview of the inference process in FreeDA.</strong> Local (region-level) and global similarities
            are computed by employing, respectively, visual self-supervised and multimodal contrastive embedding spaces,
            and by comparing them with input texts and prototypes, built during the off-line stage.
          </div>
        </div>
      </div>
      <!--/ Method. -->

      </br></br>

      <!-- Qualitative. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Results</h2>
          <div class="content has-text-centered">
            <img src="./static/images/qualitatives.png" class="center">
            </br></br>
            Qualitative results of <strong>FreeDA</strong> with and without global similarities and superpixels.
          </div>
        </div>
      </div>
      <!--/ Qualitative. -->
      </br></br>
      <!-- Explainability. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Prototypes Examples</h2>
          <div class="content has-text-centered">
            <img src="./static/images/explainability.png" class="center">
            </br></br>
            <strong>Examples of retrieved prototypes for a specified textual category</strong>. From left to right, we
            show the original COCO caption, the
            corresponding generated image, the attribution map, and the binarized mask (area highlighted in red).
          </div>
        </div>
      </div>
      <!--/ Explainability. -->
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{barsellotti2024training
        title={Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation},
        author={Barsellotti, Luca and Amoroso, Roberto and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
        year={2024}
      }</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
